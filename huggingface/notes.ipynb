{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a8584ba",
   "metadata": {},
   "source": [
    "üìÑ Extracting Environment Variables and Importing Required Libraries\n",
    "\n",
    "fitz: Used for working with PDF files (via PyMuPDF).\n",
    "\n",
    "os: Enables interaction with the operating system, such as reading environment variables.\n",
    "\n",
    "numpy: Useful for numerical computations.\n",
    "\n",
    "json: Handles reading and writing JSON data.\n",
    "\n",
    "dotenv: Loads environment variables from a .env file.\n",
    "\n",
    "InferenceClient from huggingface_hub: Allows interaction with Hugging Face's inference endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c8ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a69f2",
   "metadata": {},
   "source": [
    "üîó Initializing the Hugging Face Inference Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c3b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = InferenceClient(\n",
    "    provider=\"hf-inference\",\n",
    "    api_key= token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aa50230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. \n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text\n",
    "\n",
    "\n",
    "# Define the path to the PDF file\n",
    "pdf_path = \"C:/Users/Eray/Desktop/RAG/rag-learning/data/AI_Information.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(extracted_text[0:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ae1d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This i\n",
      "s is a\n",
      "s a te\n",
      " test \n",
      "st exa\n",
      "exampl\n",
      "mple f\n",
      "e for \n",
      "or chu\n",
      "chunk_\n",
      "nk_tex\n",
      "text f\n",
      "t func\n",
      "unctio\n",
      "tion.\n",
      "n.\n"
     ]
    }
   ],
   "source": [
    "t_chunks = chunk_text(\"This is a test example for chunk_text function.\", 6, 3)\n",
    "\n",
    "for i in t_chunks:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74c490f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 108\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, \n"
     ]
    }
   ],
   "source": [
    "text_chunks = chunk_text(extracted_text, 512, 200)\n",
    "\n",
    "# # Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# # Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb208a",
   "metadata": {},
   "source": [
    "This snippet iterates over a list of texts (text_list) and generates embeddings for each text using the Hugging Face Inference API client\n",
    "\n",
    "The raw output result from the Hugging Face API is converted into a NumPy array called embedding_vector.\n",
    "\n",
    "This allows for efficient numerical operations on the embedding.\n",
    "\n",
    "The resulting vector is then appended to the embeddings list for later use, such as similarity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71689a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text_list):\n",
    "        \n",
    "    embeddings = []\n",
    "    for text in text_list:\n",
    "        result = embedding_client.feature_extraction(\n",
    "            text,\n",
    "            model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        embedding_vector = np.array(result)\n",
    "\n",
    "        embeddings.append(embedding_vector)\n",
    "        \n",
    "        \n",
    "    return embeddings   # ::--> [nparray, nparray2, ...] \n",
    "\n",
    "\n",
    "\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97d9647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "108\n",
      "[-0.02458423  0.00881549  0.00182075  0.0167284  -0.0075     -0.05978368\n",
      "  0.08769932  0.04123058 -0.04862984  0.0482662  -0.06162594 -0.02015465]\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(type(response[0]))\n",
    "print(len(response))\n",
    "print(response[0][0:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc9812",
   "metadata": {},
   "source": [
    "üìê Calculating Cosine Similarity Between Two Vectors\n",
    "\n",
    "The cosine_similarity function computes the cosine similarity metric, which measures the cosine of the angle between two vectors in a multi-dimensional space.\n",
    "\n",
    "    Inputs: Two NumPy arrays (vec1 and vec2) representing vectors.\n",
    "\n",
    "    Process: Calculates the dot product of the vectors divided by the product of their magnitudes (norms).\n",
    "\n",
    "    Output: A float value between -1 and 1 indicating similarity:\n",
    "\n",
    "        1 means vectors are identical in direction.\n",
    "\n",
    "        0 means vectors are orthogonal (no similarity).\n",
    "\n",
    "        -1 means vectors are diametrically opposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6420b451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): The first vector.\n",
    "    vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804eb1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings([query])[0]\n",
    "    \n",
    "    # ::--> [nparray(query_embedding)] , len = 1\n",
    "    \n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "99771d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('C:/Users/Eray/Desktop/RAG/rag-learning/data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "query = data[0]['question']\n",
    "\n",
    "# Perform semantic search to find the top 2 most relevant text chunks for the query\n",
    "top_chunks = semantic_search(query, text_chunks, response, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e2dea0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "Context 1:\n",
      "he Future of Artificial Intelligence \n",
      "The future of AI is likely to be characterized by continued advancements and broader adoption \n",
      "across various domains. Key trends and areas of development include: \n",
      "Explainable AI (XAI) \n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. XAI \n",
      "techniques are being developed to provide insights into how AI models make decisions, \n",
      "enhancing trust and accountability. \n",
      "AI at the Edge \n",
      "AI at the edge involves processing data locally on devices, \n",
      "=====================================\n",
      "Context 2:\n",
      "p learning models, as well \n",
      "as exploring new architectures and training techniques. \n",
      "Explainable AI (XAI) \n",
      "Explainable AI (XAI) aims to make AI systems more transparent and understandable. Research in \n",
      "XAI focuses on developing methods for explaining AI decisions, enhancing trust, and improving \n",
      "accountability. \n",
      "AI and Neuroscience \n",
      "The intersection of AI and neuroscience is a promising area of research. Understanding the \n",
      "human brain can inspire new AI algorithms and architectures, while AI can provide ins\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Print the query\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# Print the top 2 most relevant text chunks\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "75c35ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"Qwen/Qwen2.5-7B-Instruct-1M\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response.\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_client = InferenceClient(\n",
    "        provider=\"featherless-ai\",\n",
    "        api_key=os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "    )\n",
    "    \n",
    "    response = chat_client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "30f9e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='stop', index=0, message=ChatCompletionOutputMessage(role='assistant', content='Explainable AI (XAI) is aimed at making AI systems more transparent and understandable. This approach is crucial because it enhances trust and accountability by providing insights into how AI models make decisions.', tool_call_id=None, tool_calls=None), logprobs=None)], created=1751303621145, id='Bkp1By', model='Qwen/Qwen2.5-7B-Instruct-1M', system_fingerprint='', usage=ChatCompletionOutputUsage(completion_tokens=39, prompt_tokens=287, total_tokens=326), object='chat.completion')\n",
      "<class 'huggingface_hub.inference._generated.types.chat_completion.ChatCompletionOutput'>\n",
      "Qwen/Qwen2.5-7B-Instruct-1M\n",
      "<class 'list'>\n",
      "Explainable AI (XAI) is aimed at making AI systems more transparent and understandable. This approach is crucial because it enhances trust and accountability by providing insights into how AI models make decisions.\n"
     ]
    }
   ],
   "source": [
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "\n",
    "# # Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "print(ai_response)\n",
    "print(type(ai_response))\n",
    "\n",
    "print(ai_response.model)\n",
    "print(type(ai_response.choices))\n",
    "\n",
    "print(ai_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "36833f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1\n",
      "\n",
      "The AI response accurately captures the essence of Explainable AI (XAI), emphasizing its goal of transparency and understandability, as well as its importance for trust, accountability, and fairness. The response closely aligns with the true response provided, hence a score of 1 is appropriate.\n"
     ]
    }
   ],
   "source": [
    "# Define the system prompt for the evaluation system\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# Create the evaluation prompt by combining the user query, AI response, true response, and evaluation system prompt\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# Generate the evaluation response using the evaluation system prompt and evaluation prompt\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# Print the evaluation response\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
